<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    em {
    color: #1772d0;
     }

    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    td,th,tr,p,a {
    font-family:  'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px;
    font-weight: 300;
    }
   /*<D-r> body {
    background-color: rgb(253, 253, 253) ;
    margin-top: 50px;
    } */
    body { 
	margin:5px 0; 
	padding:0; 
	font-size: 100%;
	font-family: "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
	color:#FFF;  
	background-color:#eee;
	line-height: 1.4em; 
	/*background : #E4E4E4 url(bg.gif) repeat-x;*/
/* 	background: #E4E4E4 url(bg_light.gif) repeat-x; */
	background: #FFFFFF url(bc-website-color.gif) repeat-x;
    }
p { 
	margin: 0 0 5px 0; 
	padding: 0; 
	color: #404241; 
	background: inherit;
	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 95%;
    font-weight: 400;
}

hr {
   border: 0;
   height: 1px;
   color: #eee;
   background-color: #eee;
}

a { 
	background: inherit; 
	text-decoration:none;
	font-size:95%;
       color: #191970;

}

a:hover { 
	background: inherit;
	text-decoration: none;

}

h1 { 
	padding:0; 
	margin:0; 
	color: #434A55; 
	background: inherit;
	font-family: serif; 
	/*font-size: 22px;*/
	/*letter-spacing: 0px;*/
}

h1 a {
	color: #191970; 
	background: inherit;
}

h2 { 
	background-color: inherit; 
	color:#191970; 
	margin: 10px 20px 10px 0px; 
	padding:15px 0px 0 0px; 
	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: 500;

}

h2 a { 
	/*background-color:#08152E;*/ 
	/*background-color:#E4E4E4;*/
	background-color:#FFFFFF;
}

ul { 	margin: 0 0 20px 0; 
	padding : 0; 
	list-style : none; 
        color: #555;
}
	
li { 
	float: left;
	font-weight: bold;
	margin: 10px 0 8px 0;
	padding: 0 0 0 5px;
	font-size: 95%%;
	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
	font-weight: 400;
        color: #404241;

}

li a { 
font-size: 95%;
font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
font-weight: 400;
color: #191970; }
li a:hover {text-decoration: none; background: inherit url(select.gif) no-repeat center top;padding: 2px 4px;
	background-position: 100% 100%;color: #DBBC58;padding: 4px 8px; background-color:#191970; border-radius: 25px;display: run-in;}



    strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    color: #191970;

    }

    heading {
    font-family:  'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 23px;
    color: #191970;

    }

    papertitle {
    font-family:  'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px;
    font-weight: 700
    }

    name {
    font-family:  "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
    font-size: 35px;
    font-weight: bold;
    }

    
    span.highlight {
        background-color: #ffffd0;
    }

.some_list { 
	font-size: 93%;
	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
	font-weight: 400;
	/*float: none;*/
	list-style-type: circle;
	padding : 0px; 
	margin: 0 0 10px 30px; 
}
  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <title>Adji Bousso Dieng </title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Roboto:300,400,500,700,900,100italic,100,300,300italic,400italic,500italic,900italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="70%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <p>
          <name>Adji Bousso Dieng</name>
        </p>
        <p>
        <img src="Adji-profile.jpg" width="30%">
        </p>
          <p>
          <a href="Adji-CV.pdf">CV</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=ZCniP_MAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/diengadji45/?locale=en_US"> LinkedIn </a> &nbsp/&nbsp
          <a href="https://github.com/adjidieng">Github</a> &nbsp/&nbsp
          <a href="https://twitter.com/adjiboussodieng">Twitter</a>  &nbsp/&nbsp
          <a> Email:</a>  abd2141 at columbia dot edu 
        </p>
        </td>
       </tr>
      </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
        <td width="100%" valign="middle">

        <p>I am a Ph.D student in the department of <a href="http://stat.columbia.edu">Statistics</a>
at <a href="http://columbia.edu">Columbia University</a> where I am jointly being advised by <a href="http://www.cs.columbia.edu/~blei/">David Blei</a>
and  <a href="http://www.columbia.edu/~jwp2128/">John Paisley</a>. In my research, I work on combining probabilistic graphical modeling and deep learning to design models for structured high-dimensional data such as text. I also work on variational methods as an inference framework for fitting these models.
       </p>
        <p> &nbsp </p>
        <p>
         Prior to joining Columbia I worked as a Junior Professional Associate at the World Bank. I did my undergraduate training in France where I attended Lycee Henri IV and Telecom ParisTech--France's Grandes Ecoles system. I hold a Diplome d'Ingenieur from Telecom ParisTech and spent the third year of Telecom ParisTech's curriculum at Cornell University where I earned a Master in Statistics.</p>
        </td> 
      </tr>
      </table>
 
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
        <ul style="list-style-type:circle">
  <li class="some_list"> Sep 2019: I was very glad to serve as Area Chair for the <a href="https://wimlworkshop.org/2019/">WIML</a> and <a href="https://blackinai.github.io/workshop/2019/cfp/">Black in AI</a> workshops collocated with <a href="https://nips.cc/Conferences/2019">NeurIPS 2019</a>. </li>
  <li class="some_list"> Sep 2019: I have been named a <a href="http://www.umiacs.umd.edu/about-us/news/machine-learning-center-launches-program-supporting-female-researchers">Rising Star in Machine Learning</a> by the University of Maryland's <a href="https://ml.umd.edu/">Center for Machine Learning</a>. </li>
  <li class="some_list"> Aug 2019: I am co-organizing the <a href="http://approximateinference.org/">2nd Symposium on Advances in Approximate Bayesian Inference</a>. </li>
  <li class="some_list"> Aug 2019: I will be giving a two-hour lecture on deep generative models at this year's <a href="http://www.deeplearningindaba.com/speakers-2019.html">Deep Learning Indaba</a>. Here is a link to the <a href="Papers/dgms.pdf">slides</a>  </li>
  <li class="some_list"> May 2019: I co-organized an ICLR workshop on <a href="https://deep-gen-struct.github.io/"> deep generative models and structured data</a>. </li>
  <li class="some_list"> Apr 2019: I have been named a <a href="https://ai.googleblog.com/2019/09/announcement-of-2019-fellowship.html">Google PhD Fellow in Machine Learning!</a> </li>
	<li class="some_list"> Sep 2018: I will be spending this Fall semester at <a href="https://deepmind.com">DeepMind, London</a>. </li>
	<li class="some_list"> July 2018: Check out my <a href="https://twimlai.com/twiml-talk-160-designing-better-sequence-models-with-rnns-with-adji-bousso-dieng/"> TWiMLAI podcast interview </a> with Sam Chanrington. </li>
        <li class="some_list"> May 2018:   I am excited to be interning with Yann LeCun at <a heref="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research</a> this summer.</li>
        <li class="some_list"> May 2018:   I co-authored two papers that are appearing at this year's ICML: "Augment and Reduce: Stochastic Inference for Large Categorical Distributions" and "Noisin: Unbiased Regularization for Recurrent Neural Networks". </li>
        <li class="some_list"> Feb 2018:   I will be part of the <a href="https://www.youtube.com/watch?v=z35FvTQXJCU"> Women Techmakers 2018 Summit panel </a> at Google, New York. </li>
        <li class="some_list"> Feb 2018:   I will be giving a <a href="https://www.nyas.org/events/2018/12th-annual-machine-learning-symposium/"> spotlight talk at the NYAS ML Symposium </a> about our work on noise-based regularizers for recurrent neural networks. </li> 
	<li class="some_list"> Feb 2018:   I am a finalist for the <a href="https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/announcing-2018-ai-fellows">Open Philanthropy Project AI Fellowship</a>! </li>
	<li class="some_list"> Dec 2017:   I am glad to be part of the mentors for this year's <a href="https://wimlworkshop.org/2017/program/#1480549898816-ca283fe3-29b613">Women in Machine Learning Mentorship Roundtable</a>. </li>
        </ul>         
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading> Selected Invited Talks </heading>
         <ul  style="list-style-type:circle">

        <li class="some_list"> Berkeley AI Research Seminar Series, Berkeley, CA, October 2019 </li>  
        <li class="some_list"><a href="https://www.ipam.ucla.edu/programs/workshops/workshop-ii-interpretable-learning-in-physical-sciences/?tab=speaker-list">IPAM Workshop on Interpretable Learning in Physical Systems</a>, Los Angeles, CA, October 2019 </li> 
        <li class="some_list"> University of Maryland's Rising Stars in Machine Learning seminar, College Park, MD, September 2019 </li>
        <li class="some_list"> Yahoo Research Seminar Series, New York, NY, July 2019 </li> 
        <li class="some_list"><a href="https://www.meetup.com/NYC-Artificial-Intelligence-Machine-Learning/events/260831267/">New York Machine Learning and Artificial Intelligence Meetup</a>, New York, NY, June 2019 </li> 
	      <li class="some_list"> <a href="https://cds.nyu.edu/text-data-speaker-series/">NYU Text as Data Seminar Series</a>, New York, NY, March 2019 </li> 
        <li class="some_list"> <a href="https://www.meetup.com/it-IT/South-England-Natural-Language-Processing-Meetup/events/258081065/">South England Natural Language Processing Meetup</a>, London, UK, January 2019 </li>
        <li class="some_list"> Microsoft Research Cambridge, Cambridge, UK, January 2019 </li>
        <li class="some_list"> <a href="http://www.cs.tufts.edu/t/colloquia/current/?event=1211">Tufts University CS Colloquium </a>, Medford, MA, April 2018 </li>
        <li class="some_list"> Harvard University NLP Group Meeting, Cambridge, MA, April 2018 </li>
        <li class="some_list"> <a href="https://nlp.stanford.edu/seminar/details/adieng.shtml">Stanford University NLP Seminar </a>, Stanford, CA, April 2018</li>
        <li class="some_list"> <a href="https://www.nyas.org/events/2018/12th-annual-machine-learning-symposium/?tab=agenda"> New York Academy of Science ML Symposium </A>, NY, March 2018 </li>
        <li class="some_list"> <a href="https://people.cs.umass.edu/~mlfriend/pmwiki/pmwiki.php?n=Main.DeepSequenceModelsContextRepresentationRegularizationAndApplicationToLanguage">
Machine Learning and Friends Seminar</a>, UMass, Amherst, MA, February 2018 </li>
        <li class="some_list"><a href="https://blackinai.github.io/workshop/2017/programs/">Black in AI Workshop</a>, Long Beach, CA, December 2017 </li>
        <li class="some_list"> MSR AI, Microsoft Research, Redmond, WA, August 2017 </li>
        <li class="some_list"> SSLI Lab, University of Washington, Seattle, WA, August 2017 </li>
        <li class="some_list"> <a href="http://deeploria.gforge.inria.fr/seminars">DeepLoria, Loria Laboratory</a>, Nancy, France, April 2017</li>
        <li class="some_list"> AI With The Best, Online, April 2017 </li>
        <li class="some_list"> OpenAI, San Francisco, CA, January 2017 </li>
	<li class="some_list"> IBM TJ Watson Research, Yorktown Heights, NY, December 2016 </li>
	<li class="some_list"> Microsoft Research, Redmond, WA, August 2016  </li>
        </ul>         
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p> My goal as a Machine Learning researcher is twofold. My first goal is to combine deep learning and probabilistic graphical modeling to design models that are expressive and powerful enough to capture meaningful representations of high-dimensional structured data. My second goal is to develop efficient, scalable, and generic algorithms for learning with these models. Achieving these two goals will benefit many applications. 
          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">

    <tr>
      <td width="25%">
        <img src='Images/presgan.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
    <a href="Papers/PRESGAN.pdf">
            <papertitle>Prescribed Generative Adversarial Networks</papertitle>
    </a>
    <br>
          <strong>Adji B. Dieng</strong>,
          <a href="http://franrruiz.github.io/index.html">Francisco R. J. Ruiz</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a>,
          <a href="http://www2.aueb.gr/users/mtitsias/">Michalis Titsias</a>
    <br>
        <em>Under submission at Journal of Machine Learning Research</em>  <br>
        <a href="https://arxiv.org/abs/1906.05850">arxiv</a>
        /
        <a href="https://github.com/adjidieng/PresGANs">Code</a>
         / 
        </p>
        <p>This paper describes a solution to two important problems in the GAN literature: (1) How can we maximize the entropy of the generator of a GAN to prevent mode collapse? (2) How can we evaluate predictive log-likelihood for GANs to assess how they generalize to new data? Key ingredients: noise, entropy regularization, and Hamiltonian Monte Carlo. </p>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='Images/rem.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
    <a href="Papers/REM.pdf">
            <papertitle>Reweighted Expectation Maximization</papertitle>
    </a>
    <br>
          <strong>Adji B. Dieng</strong>,
          <a href="http://www.columbia.edu/~jwp2128/">John Paisley</a> <br>
        <em>Under submission at Journal of Machine Learning Research</em>  <br>
        <a href="https://arxiv.org/abs/1906.05850">arxiv</a>
        /
        <a href="https://github.com/adjidieng/REM">Code</a>
         / 
        </p>
        <p>Maximum likelihood in deep generative models is hard. The typical workaround is variational inference (VI) which maximizes a lower bound to the log marginal likelihood of the data. VI introduces an undesirable amortization gap and often causes latent variable collapse. We propose to use expectation maximization (EM) instead. Importantly, we separate posterior inference and model fitting. To fit the model we leverage moment matching to learn rich proposals to estimate the EM objective. Posterior inference is done after the model is fitted. This two-step procedure shies away from the current VAE approach of bundling together model fitting and posterior inference. Turns out EM learns better deep generative models than VI as measured by predictive log-likelihood.</p>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='Images/detm.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
    <a href="Papers/DETM.pdf">
            <papertitle>The Dynamic Embedded Topic Model</papertitle>
    </a>
    <br>
          <strong>Adji B. Dieng*</strong>,
          <a href="http://franrruiz.github.io/index.html">Francisco R. J. Ruiz*</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a> <br>
        <em>Submitted to International Conference on Artificial Intelligence and Statistics (AISTATS), 2020</em>  <br>
        <a href="https://arxiv.org/abs/1907.05545">arxiv</a>
        /
        <a href="https://github.com/adjidieng/DETM">Code</a>
         / 
        </p>
        <p>An extension of the Embedded Topic Model to corpora with temporal dependencies. The DETM models each word with a categorical distribution whose parameter is given by the inner product between the word embedding and an embedding representation of its assigned topic at a particular time step. The word embeddings allow the DETM to generalize to rare words. The DETM learns smooth topic trajectories by defining a random walk prior over the embeddings of the topics. The DETM is fit using structured amortized variational inference with LSTMs.</p>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='Images/etm.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
    <a href="Papers/ETM.pdf">
            <papertitle>Topic Modeling in Embedding Spaces</papertitle>
    </a>
    <br>
          <strong>Adji B. Dieng</strong>,
          <a href="http://franrruiz.github.io/index.html">Francisco R. J. Ruiz</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a> <br>
        <em>Under review at Transactions of the Association for Computational Linguistics (TACL), 2019</em>  <br>
        <a href="https://arxiv.org/abs/1907.04907">arxiv</a>
        /
        <a href="https://github.com/adjidieng/ETM">Code</a>
         / 
        </p>
        <p>Define words and topics in the same embedding space. Form a generative model of documents that defines the likelihood of a word as a Categorical whose natural parameter is the dot product between the word embedding and its assigned topic's embedding. The resulting Embedded Topic Model (ETM) learns interpretable topics and word embeddings and is robust to large vocabularies that include rare words and stop words.</p>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='Images/skipvae.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
	  <a href="Papers/skipvae.pdf">
            <papertitle>Avoiding Latent Variable Collapse with Generative Skip Models</papertitle>
	  </a>
	  <br>
          <strong>Adji B. Dieng</strong>,
          <a href="http://www.people.fas.harvard.edu/~yoonkim/">Yoon Kim</a>,
          <a href="http://nlp.seas.harvard.edu/rush.html">Alexander M. Rush</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a> <br>
        <em>International Conference on Artificial Intelligence and Statistics (AISTATS), 2019</em> (Submitted) <br>
        <a href="https://arxiv.org/abs/1807.04863">arxiv</a>
         / 
        </p>
        <p>One of the current staples of unsupervised representation learning is variational autoencoders (VAEs). However they suffer from a problem known as "latent variable collapse". Our paper proposes a simple solution that relies on skip connections. This solution leads to the Skip-VAE--a deep generative model that avoids latent variable collapse. The decoder of a Skip-VAE is a neural network whose hidden states--at every layer--condition on the latent variables. This results in a stronger dependence between observations and their latents and therefore avoids latent variable collapse.</p>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='Images/overfitting.png' width="320" height="250">
      </td>
      <td valign="top" width="75%">
          <p>
          <a href="Papers/noisin.pdf">
            <papertitle>Noisin: Unbiased Regularization for Recurrent Neural Networks</papertitle>
          </a>
          <br>
          <strong>Adji B. Dieng</strong>,
          <a href="https://cims.nyu.edu/~rajeshr/">Rajesh Ranganath</a>,
          <a href="https://jaan.io/">Jaan Altosaar</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a> <br>
        <em>International Conference on Machine Learning (ICLR), 2018</em> <br><D-r>
        <a href="https://arxiv.org/abs/1805.01500">arxiv</a>
        /
        <a href="Papers/noisin_slides.pdf">Slides</a>
        </p>
        <p>Recurrent neural networks are very effective at modeling sequential data. However they tend to have very high capacity and overfit very easily. We propose a new regularization method called Noisin. Noisin relies on the notion of "unbiased" noise injection. Noisin is an explicit regularizer--it's objective function can be decomposed as the original objective for the deterministic RNN and a non-negative data-dependent term. Noisin significantly outperforms Dropout on both the Penn TreeBank and the Wikitext-2 datasets on a language modeling task. </p>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='Images/a_and_r.png' width="280" height="130">
      </td>
      <td valign="top" width="75%">
        <p>
	  <a href="Papers/augment-reduce.pdf">
            <papertitle>Augment and Reduce: Stochastic Inference for Large Categorical Distributions</papertitle>
	  </a>
	  <br>
          <a href="http://franrruiz.github.io/index.html">Francisco J. R. Ruiz</a>,
	  <a href="http://www2.aueb.gr/users/mtitsias/">Michalis Titsias</a>,
	  <strong>Adji B. Dieng</strong>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a> <br>
        <em>International Conference on Machine Learning (ICML), 2018</em> <br>
        <a href="https://arxiv.org/abs/1802.04220">arxiv</a>
        /
        <a href="Papers/a_and_r_slides.pdf">Slides</a>
        /
        <a href="https://github.com/franrruiz/augment-reduce">Code</a>
        </p>
        <p>Categorical distributions are ubiquitous in Statistics and Machine Learning. One wide parameterization of a categorical distribution is the softmax. However softmax does not scale well when there are many categories. We propose a method called A&R that scales learning with categorical distributions. A&R is built on two ideas: latent variable augmentation and stochastic variational expectation maximization.</p>
      </td>
    </tr>
		
    <tr>
      <td width="25%">
        <img src='Images/topicrnn.png' width="280" height="230">
      </td>
      <td valign="top" width="75%">
       <p>
        <a href="Papers/topicrnn.pdf">
        <papertitle>TopicRNN: A Recurrent Neural Network With Long-Range Semantic Dependency</papertitle>
        </a>
        <br>
        <strong>Adji B. Dieng</strong>,
        <a href="http://">Chong Wang</a>,
        <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao</a>,
        <a href="http://www.columbia.edu/~jwp2128/">John Paisley</a>,<br>
        <em>International Conference on Learning Representations (ICLR), 2017 </em> <br>
        <a href="https://arxiv.org/abs/1611.01702">arxiv</a> /
        <a href="Papers/topicrnn_poster.pdf">Poster</a>
        /
        <a href="Papers/topicrnn_slides.pdf">Slides</a>
        </p>
        <p>One challenge in modeling sequential data with RNNs is the inability to capture long-term dependencies. In natural language these long-term dependencies come in the form of semantic dependencies. TopicRNN is a deep generative model of language that marries RNNs and topic models to capture long-term dependencies. The RNN component of the model captures syntax while the topic model component captures semantic. The topic model and the RNN parameters are learned jointly using amortized variational inference.</p>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='Images/sandwich.png' width="280" height="230">
      </td>
      <td valign="top" width="75%">
        <p><a href="Papers/chivi.pdf">
          <papertitle>Variational Inference via Chi Upper Bound Minimization</papertitle></a><br>
          <strong>Adji B. Dieng</strong>,
          <a href="http://dustintran.com/">Dustin Tran</a>,
          <a href="https://cims.nyu.edu/~rajeshr/">Rajesh Ranganath</a>,
          <a href="http://www.columbia.edu/~jwp2128/">John Paisley</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a> <br>
          <em>Neural Information Processing Systems (NIPS), 2017</em> <br>
         <a href="https://arxiv.org/abs/1611.00328">arxiv</a> 
        /<D-r>
        <a href="Papers/chivi_poster.pdf">Poster</a>
        /
        <a href="Papers/chivi_slides.pdf">Slides</a>
        <p>Variational inference is an efficient approach for estimating posterior distributions. It consists in positing a family of distributions and finding the distribution in this family that better approximates the true posterior. The criterion for learning is a divergence measure. The most used divergence is the Kullback-Leibler (KL) divergence. However minimizing the KL leads to approximations that underestimate posterior uncertainty. Our paper proposes the Chi-divergence for variational inference. This divergence leads to an upper bound of the model evidence (called CUBO) and overdispersed posterior approximations. CUBO can be used alongside the usual ELBO to sandwich-estimate the model evidence.</p>
        </td>
      </tr>

     <tr >
      <td width="25%">
      <img src="Images/edward.png" width="200" height="200">
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="Papers/edward.pdf">
            <papertitle>Edward: A Library for Probabilistic Modeling, Inference, and Criticism</papertitle>
          </a>
          <br>
	  <a href="http://dustintran.com">Dustin Tran</a>,
	  <a href="">Alp Kucukelbir</a>,
          <strong>Adji B. Dieng</strong>,
	  <a href="http://maja-rita-rudolph.com">Maja Rudolph</a>,
	  <a href="http://dawenl.github.io">Dawen Liang</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a>	<br>	
        <a href="https://arxiv.org/abs/1610.09787">arxiv</a>
        </p>
        <p>A tensorflow-based library for probabilistic programming.</p>
      </td>
    </tr>

       </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
      <tr>
        <td>
        <heading>Teaching</heading>
      /
      <p> I have been a teaching assistant for the following courses at Columbia University.</p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="25">
      <tr>
        <td width="25%"><img src="Images/teaching.jpg" alt="pacman" width="180" height="180"></td>
        <td width="75%" valign="right">
        <p>
          <papertitle>Statistical Machine Learning - Spring 2019</papertitle>
          <br><br>
          <papertitle>Advanced Data Analysis - Fall 2017</papertitle>
          <br><br>
          <papertitle>Statistical Methods for Finance - Spring 2016</papertitle>
          <br><br>
          <papertitle>Probability and Statistics for Data Science - Fall 2015</papertitle>
          <br><br>
          <papertitle>Linear Regression Models - Spring 2015</papertitle>
          <br><br>
          <papertitle>Probability - Fall 2014</papertitle>
          <br>

        </p>
        </td>
      </tr>
      </table>
    </td>
    </tr>
  </table>
  </body>
</html>
